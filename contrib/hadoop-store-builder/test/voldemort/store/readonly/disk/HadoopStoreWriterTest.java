package voldemort.store.readonly.disk;

import java.io.File;
import java.io.IOException;
import java.security.MessageDigest;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import com.google.common.collect.Lists;
import junit.framework.Assert;

import org.apache.commons.io.FileUtils;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.OutputCollector;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;
import org.junit.runners.Parameterized.Parameters;

import voldemort.ServerTestUtils;
import voldemort.TestUtils;
import voldemort.routing.RoutingStrategyType;
import voldemort.store.StoreDefinition;
import voldemort.store.readonly.checksum.CheckSum;
import voldemort.store.readonly.mr.AbstractCollectorWrapper;
import voldemort.store.readonly.mr.AbstractStoreBuilderConfigurable;
import voldemort.store.readonly.mr.BuildAndPushMapper;
import voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob;
import voldemort.store.readonly.utils.ReadOnlyTestUtils;
import voldemort.utils.ByteUtils;
import voldemort.xml.ClusterMapper;
import voldemort.xml.StoreDefinitionsMapper;

@RunWith(Parameterized.class)
/**
 * Tight assumptions this test makes: nodeId = 0, partitionId = 0,
 * replicaType = 0, Number of chunks = 2
 *
 * So files generated will be like:
 * (if saveKeys == true){
 *   0_0_<chunkId>.data // where chunkId can be 0 or 1 since num of chunks = 2.
 * }else{
 *   0_<chunkId>.data // where chunkId can be 0 or 1 since num of chunks = 2.
 * }
 *
 * The only compression type allowed is gzip.
 *
 * Also this test suite does not check for checksum types. It assumes a
 * default checksum type of NONE.
 *
 * This test also does not verify the shuffling/distribution of keys.
 */
public class HadoopStoreWriterTest {

    private JobConf conf = null;
    private HadoopStoreWriter hadoopStoreWriterPerBucket = null;
    private File tmpOutPutDirectory = null;
    private boolean saveKeys;
    private File dataFile, indexFile;
    private int numKeys = 1000;
    private int numChunks = 2;
    private BuildAndPushMapper mapper;
    private TestCollectorWrapper testCollectorWrapper;
    private TestCollector testCollector;

    @Parameters
    public static Collection<Object[]> configs() {
        return Arrays.asList(new Object[][] { { true }, { false } });
    }

    public HadoopStoreWriterTest(boolean saveKeys) {
        this.saveKeys = saveKeys;
    }

    private void init() {
        tmpOutPutDirectory = TestUtils.createTempDir();

        // Setup before each test method
        conf = new JobConf();
        conf.setInt(AbstractStoreBuilderConfigurable.NUM_CHUNKS, numChunks);
        conf.set("final.output.dir", tmpOutPutDirectory.getAbsolutePath());
        conf.set("mapred.output.dir", tmpOutPutDirectory.getAbsolutePath());
        conf.set("mapred.task.id", "1234");

        /**
         * We don't have to test different types of checksums. That's covered in
         * {@link voldemort.store.readonly.checksum.CheckSumTests}.
         */
        conf.set(VoldemortBuildAndPushJob.CHECKSUM_TYPE, CheckSum.CheckSumType.NONE.name());

        // generate a list of storeDefinitions.
        List<StoreDefinition> storeDefList = Lists.newArrayList(
                ServerTestUtils.getStoreDef("test",
                                            1, // Replication Factor 1, since we are testing a one node "cluster"
                                            1, 1, 1, 1, // preferred/required reads/writes all at 1
                                            RoutingStrategyType.CONSISTENT_STRATEGY.toString()));
        String storesXML = new StoreDefinitionsMapper().writeStoreList(storeDefList);
        conf.set("stores.xml", storesXML);
        String clusterXML = new ClusterMapper().writeCluster(ServerTestUtils.getLocalCluster(1));
        conf.set("cluster.xml", clusterXML);

        // We leverage the real mapper used in the BnP job to generate data with the proper format
        mapper = new BuildAndPushMapper();
        mapper.configure(conf);
        testCollector = new TestCollector();
        testCollectorWrapper = new TestCollectorWrapper();
        testCollectorWrapper.setCollector(testCollector);
    }

    private void cleanUp() throws IOException {
        if(tmpOutPutDirectory != null) {
            FileUtils.deleteDirectory(tmpOutPutDirectory);
        }
    }

    /**
     * The purpose of this class is to provide an easy way to access the key/values
     * generated by the {@link voldemort.store.readonly.mr.BuildAndPushMapper}.
     */
    class TestCollector implements OutputCollector<BytesWritable, BytesWritable> {
        public BytesWritable key;
        public List<BytesWritable> values = Lists.newArrayList();

        @Override
        public void collect(BytesWritable key, BytesWritable value) throws IOException {
            if (!key.equals(this.key)) {
                this.key = key;
                values.clear();
            }
            values.add(value);
        }
    }

    /**
     * This abstraction is necessary in order to be able to pass in the
     * {@link voldemort.store.readonly.disk.HadoopStoreWriterTest.TestCollector}.`
     */
    class TestCollectorWrapper extends AbstractCollectorWrapper<TestCollector> {
        @Override
        public void collect(byte[] key, byte[] value) throws IOException {
            BytesWritable keyBW = new BytesWritable(), valueBW = new BytesWritable();
            keyBW.setSize(key.length);
            keyBW.set(key, 0, key.length);
            valueBW.setSize(value.length);
            valueBW.set(value, 0, value.length);
            getCollector().collect(keyBW, valueBW);
        }
    }

    private void generateUnCompressedFiles(boolean saveKeys) throws IOException {
        conf.setBoolean(VoldemortBuildAndPushJob.SAVE_KEYS, saveKeys);
        conf.setBoolean(VoldemortBuildAndPushJob.REDUCER_OUTPUT_COMPRESS, false);
        hadoopStoreWriterPerBucket = new HadoopStoreWriter();
        hadoopStoreWriterPerBucket.conf(conf);

        for(int i = 0; i < numKeys; i++) {
            String key = "test_key_" + i;
            String value = "test_value_" + i;
            mapper.map(key.getBytes(), value.getBytes(), testCollectorWrapper);
            hadoopStoreWriterPerBucket.write(testCollector.key, testCollector.values.iterator(), null);
        }
        hadoopStoreWriterPerBucket.close();
    }

    private void generateExpectedDataAndIndexFileNames(boolean isGzipCompressed, int chunkId) {
        String basePath = tmpOutPutDirectory.getAbsolutePath() + File.separator + "node-0"
                + File.separator;
        String fileName = "";
        String dataFilePrefix = KeyValueWriter.DATA_FILE_EXTENSION;
        String indexFilePrefix = KeyValueWriter.INDEX_FILE_EXTENSION;
        String fileExtension = "";

        if(isGzipCompressed) {
            fileExtension = KeyValueWriter.GZIP_FILE_EXTENSION;
        }
        fileName = (saveKeys ? "0_0" : "0");
        dataFile = new File(basePath + fileName + "_" + chunkId + dataFilePrefix + fileExtension);
        indexFile = new File(basePath + fileName + "_" + chunkId + indexFilePrefix + fileExtension);
    }

    @Test
    public void testUncompressedWriter() throws IOException {
        /**
         * Very basic assertion. This test case needs improvement later.
         *
         * if(saveKeys) Assert if two files are generated - 0_0_0.data and
         * 0_0_0.index with non-zero size under <tmpOutPutDirectory>/node_0.
         *
         * else Assert if two files are generated - 0_0.data and 0_0.index with
         * non-zero size under <tmpOutPutDirectory>/node_0.
         *
         */
        init();

        generateUnCompressedFiles(saveKeys);

        for(int i = 0; i < numChunks; i++) {
            generateExpectedDataAndIndexFileNames(false, i);
            if(dataFile.exists() && dataFile.isFile()) {
                if(dataFile.length() == 0) {
                    Assert.fail("Empty data file");
                }
            } else {
                Assert.fail("There is no data file in the expected directory");
            }
            if(indexFile.exists() && indexFile.isFile()) {
                if(indexFile.length() == 0) {
                    Assert.fail("Empty index file");
                }
            } else {
                Assert.fail("There is no data file in the expected directory");
            }
        }
        cleanUp();

    }

    @Test
    public void testCompressedWriter() throws IOException {

        /**
         * 1. Run uncompressed path for getting baseline files
         *
         * 2. then run the compressed path with the same key and value bytes
         *
         * 3. unzip the compressed files separately into a _decompressed file
         * for both data and index files
         *
         * 4. compare the original file and decompressed files for byte-by-byte
         * equality.
         */
        init();

        generateUnCompressedFiles(saveKeys);
        conf.setBoolean(VoldemortBuildAndPushJob.SAVE_KEYS, saveKeys);
        conf.setBoolean(VoldemortBuildAndPushJob.REDUCER_OUTPUT_COMPRESS, true);
        conf.setStrings(VoldemortBuildAndPushJob.REDUCER_OUTPUT_COMPRESS_CODEC, KeyValueWriter.COMPRESSION_CODEC);

        hadoopStoreWriterPerBucket = new HadoopStoreWriter();
        hadoopStoreWriterPerBucket.conf(conf);

        for(int i = 0; i < numKeys; i++) {
            String key = "test_key_" + i;
            String value = "test_value_" + i;
            mapper.map(key.getBytes(), value.getBytes(), testCollectorWrapper);
            hadoopStoreWriterPerBucket.write(testCollector.key, testCollector.values.iterator(), null);
        }

        hadoopStoreWriterPerBucket.close();

        // Generate a decompressed file for each of the compressed file and
        // check if they are equal with the original uncompressed file
        for(int i = 0; i < numChunks; i++) {

            // reset dataFile and indexFile to point to the compressed files
            // being
            // generated
            generateExpectedDataAndIndexFileNames(true, i);

            String decompressedDataFile = dataFile.getAbsolutePath() + "_decompressed";
            ReadOnlyTestUtils.unGunzipFile(dataFile.getAbsolutePath(), decompressedDataFile);

            String decompressedIndexFile = indexFile.getAbsolutePath() + "_decompressed";
            ReadOnlyTestUtils.unGunzipFile(indexFile.getAbsolutePath(), decompressedIndexFile);

            // reset data and index file to point to the uncompressed files that
            // were initially generated.
            generateExpectedDataAndIndexFileNames(false, i);
            Assert.assertTrue(ReadOnlyTestUtils.areTwoBinaryFilesEqual(dataFile,
                                                                       new File(decompressedDataFile)));
            Assert.assertTrue(ReadOnlyTestUtils.areTwoBinaryFilesEqual(indexFile,
                                                                       new File(decompressedIndexFile)));
        }
        cleanUp();
    }
}
